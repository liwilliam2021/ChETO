{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4txhu-dDfyhE",
        "outputId": "5ef57a73-7a30-43dd-c092-58aa7d86fb5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adapters\n",
            "  Downloading adapters-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers~=4.47.1 (from adapters)\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.47.1->adapters) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.47.1->adapters) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.47.1->adapters) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.47.1->adapters) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.47.1->adapters) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.47.1->adapters) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.47.1->adapters) (2025.1.31)\n",
            "Downloading adapters-1.1.0-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, adapters\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed adapters-1.1.0 transformers-4.47.1\n"
          ]
        }
      ],
      "source": [
        "!pip install adapters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alignment Code from https://github.com/Helw150/tada/blob/main/models.py"
      ],
      "metadata": {
        "id": "ShXTpEFTrejS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class GradientReversal(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        return i\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return -1 * grad_output"
      ],
      "metadata": {
        "id": "oyaWe7VQrAST"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Optional: Dummy Gradient Reversal Layer implementation\n",
        "class GradientReversal(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return -grad_output\n",
        "\n",
        "# Main model with LoRA adapters + alignment loss + critic\n",
        "class TADAWithAlignment(nn.Module):\n",
        "    def __init__(self, model_name='dccuchile/bert-base-spanish-wwm-uncased'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Base model with LoRA adapters\n",
        "        base_model = BertModel.from_pretrained(model_name)\n",
        "        for param in base_model.parameters():\n",
        "            param.requires_grad = False  # Freeze base model\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.FEATURE_EXTRACTION,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"query\", \"value\"]\n",
        "        )\n",
        "        self.bert = get_peft_model(base_model, lora_config)\n",
        "\n",
        "        # Critic network\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.critic_transform = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=12)\n",
        "        self.critic_score = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, 1)\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def produce_original_embeddings(self, input_ids, attention_mask):\n",
        "        self.eval()\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states[-1]  # Last hidden layer\n",
        "        hidden_states = hidden_states * attention_mask.unsqueeze(-1)\n",
        "        self.train()\n",
        "        return hidden_states\n",
        "\n",
        "    def critic(self, embedding):\n",
        "        mask = embedding.sum(-1) != 0  # padding mask\n",
        "        cls_token = self.critic_transform(\n",
        "            embedding.permute(1, 0, 2), src_key_padding_mask=mask\n",
        "        )[0, :, :]  # Take first token (CLS)\n",
        "        scores = self.critic_score(cls_token)\n",
        "        return scores.mean()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, original_embedding=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states[-1] * attention_mask.unsqueeze(-1)\n",
        "\n",
        "        if original_embedding is not None:\n",
        "            # Adversarial alignment mode\n",
        "            hidden_states_reversed = GradientReversal.apply(hidden_states)\n",
        "            alignment_loss = (\n",
        "                (original_embedding[:, 0, :] - hidden_states_reversed[:, 0, :])\n",
        "                .square()\n",
        "                .sum(1)\n",
        "                .mean()\n",
        "            )\n",
        "            critic_loss = self.critic(hidden_states_reversed) - self.critic(original_embedding)\n",
        "            total_alignment_loss = critic_loss - alignment_loss\n",
        "            return total_alignment_loss\n",
        "        else:\n",
        "            # Inference mode (just CLS token for downstream task)\n",
        "            return hidden_states[:, 0, :]  # Return [CLS] token"
      ],
      "metadata": {
        "id": "NPeVdrKxq3zX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the tokenizer and model.\n",
        "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "model = TADAWithAlignment()\n",
        "\n",
        "# Define optimizer and loss function.\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtiEHnt7iapM",
        "outputId": "23ed219f-17c0-4e60-d466-a0b3210e31cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Custom Dataset to hold text pairs.\n",
        "class TextPairDataset(Dataset):\n",
        "    def __init__(self, data_pairs):\n",
        "        \"\"\"\n",
        "        data_pairs: List of tuples (original_text, transformed_text)\n",
        "        \"\"\"\n",
        "        self.data = data_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_text, transformed_text = self.data[idx]\n",
        "        return {'text1': original_text, 'text2': transformed_text}\n",
        "\n",
        "# Collate function to batch and tokenize samples.\n",
        "def collate_fn(batch, tokenizer, max_length=512):\n",
        "    texts1 = [item['text1'] for item in batch]\n",
        "    texts2 = [item['text2'] for item in batch]\n",
        "    encoding1 = tokenizer(texts1, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    encoding2 = tokenizer(texts2, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    return encoding1, encoding2"
      ],
      "metadata": {
        "id": "kxSH10Zti-gU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "# Load JSON dataset.\n",
        "with open('top_n_chilean_examples.json', 'r', encoding='utf-8') as f:\n",
        "    data_json = json.load(f)\n",
        "\n",
        "# Extract paired texts: use the \"original_text\" (or the key as a fallback) and \"transformed_text\"\n",
        "data_pairs = []\n",
        "for key, value in data_json.items():\n",
        "    original_text = value.get('original_text', key)\n",
        "    transformed_text = value.get('transformed_text', None)\n",
        "    if transformed_text is not None:\n",
        "        data_pairs.append((original_text, transformed_text))\n",
        "\n",
        "# Shuffle and split into train and eval sets (e.g., 80/20 split)\n",
        "random.shuffle(data_pairs)\n",
        "split_idx = int(len(data_pairs) * 0.8)\n",
        "train_pairs = data_pairs[:split_idx]\n",
        "eval_pairs = data_pairs[split_idx:]\n",
        "\n",
        "# Create dataset objects.\n",
        "train_dataset = TextPairDataset(train_pairs)\n",
        "eval_dataset = TextPairDataset(eval_pairs)"
      ],
      "metadata": {
        "id": "2JcpeCbWjQhc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders.\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: collate_fn(batch, tokenizer)\n",
        ")\n",
        "eval_loader = DataLoader(\n",
        "    eval_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: collate_fn(batch, tokenizer)\n",
        ")"
      ],
      "metadata": {
        "id": "AFZ9KaXYjkuP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlhQntT0jsyV",
        "outputId": "7d488e98-6304-41d4-b441-27c833e0cd44"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TADAWithAlignment(\n",
              "  (bert): PeftModelForFeatureExtraction(\n",
              "    (base_model): LoraModel(\n",
              "      (model): BertModel(\n",
              "        (embeddings): BertEmbeddings(\n",
              "          (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
              "          (position_embeddings): Embedding(512, 768)\n",
              "          (token_type_embeddings): Embedding(2, 768)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder): BertEncoder(\n",
              "          (layer): ModuleList(\n",
              "            (0-11): 12 x BertLayer(\n",
              "              (attention): BertAttention(\n",
              "                (self): BertSdpaSelfAttention(\n",
              "                  (query): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (value): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): BertSelfOutput(\n",
              "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate): BertIntermediate(\n",
              "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): BertOutput(\n",
              "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (pooler): BertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (critic_transform): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (critic_score): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=384, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_alignment = 0.1\n",
        "num_epochs = 3\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for encoding1, encoding2 in train_loader:\n",
        "        input_ids1 = encoding1['input_ids'].to(device)\n",
        "        attention_mask1 = encoding1['attention_mask'].to(device)\n",
        "        input_ids2 = encoding2['input_ids'].to(device)\n",
        "        attention_mask2 = encoding2['attention_mask'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Step 1: Original frozen embeddings\n",
        "        with torch.no_grad():\n",
        "            original_embedding1 = model.produce_original_embeddings(input_ids1, attention_mask1)\n",
        "            original_embedding2 = model.produce_original_embeddings(input_ids2, attention_mask2)\n",
        "\n",
        "        # Step 2: Alignment loss (adversarial)\n",
        "        alignment_loss1 = model(input_ids1, attention_mask1, original_embedding=original_embedding1)\n",
        "        alignment_loss2 = model(input_ids2, attention_mask2, original_embedding=original_embedding2)\n",
        "\n",
        "        # Step 3: Contrastive loss\n",
        "        cls1 = model(input_ids1, attention_mask1)\n",
        "        cls2 = model(input_ids2, attention_mask2)\n",
        "        contrastive_loss = criterion(cls1, cls2)\n",
        "\n",
        "        # Step 4: Total loss\n",
        "        total_loss = contrastive_loss + lambda_alignment * (alignment_loss1 + alignment_loss2)\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += total_loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_eval_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for encoding1, encoding2 in eval_loader:\n",
        "            input_ids1 = encoding1['input_ids'].to(device)\n",
        "            attention_mask1 = encoding1['attention_mask'].to(device)\n",
        "            input_ids2 = encoding2['input_ids'].to(device)\n",
        "            attention_mask2 = encoding2['attention_mask'].to(device)\n",
        "            cls1 = model(input_ids1, attention_mask1)\n",
        "            cls2 = model(input_ids2, attention_mask2)\n",
        "            loss = criterion(cls1, cls2)\n",
        "            total_eval_loss += loss.item()\n",
        "    avg_eval_loss = total_eval_loss / len(eval_loader)\n",
        "    print(f\"Eval Loss: {avg_eval_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f'tada_alignment_epoch_{epoch+1}.pt')\n",
        "    print(f\"Model saved to tada_alignment_epoch_{epoch+1}.pt\")"
      ],
      "metadata": {
        "id": "Kil1Ua2mheWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba9a8b1-538f-49dc-b02f-94a65ded49db"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Loss: -9.9349\n",
            "Eval Loss: 0.0010\n",
            "Model saved to tada_alignment_epoch_1.pt\n",
            "Epoch 2 - Train Loss: -45.4152\n",
            "Eval Loss: 0.0003\n",
            "Model saved to tada_alignment_epoch_2.pt\n",
            "Epoch 3 - Train Loss: -132.4428\n",
            "Eval Loss: 0.0002\n",
            "Model saved to tada_alignment_epoch_3.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = data_pairs[0][0]"
      ],
      "metadata": {
        "id": "Lq-Mz6NMukKX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    input_ids = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n",
        "\n",
        "    # Inference mode (no alignment loss)\n",
        "    cls_embedding = model(input_ids, attention_mask)"
      ],
      "metadata": {
        "id": "KhXQtEVxuZxN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcIh91lDu1tW",
        "outputId": "7fde7841-d573-4c8f-b31d-86d74c97a788"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}