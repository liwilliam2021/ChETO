{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "GiduwxG4QjWp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
        "    'Connection': 'keep-alive'\n",
        "}"
      ],
      "metadata": {
        "id": "bp1-EBCWRIup"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_review_data_from_html(html):\n",
        "  stars_label = \"small stars stars-\"\n",
        "  stars_start = html.index(stars_label) + len(stars_label)\n",
        "  if (html[stars_start + 1] == '\"'):\n",
        "    stars = float(html[stars_start])\n",
        "  else:\n",
        "    stars = float(html[stars_start] + \".\" + html[stars_start+1])\n",
        "\n",
        "  review_span_idx = html.index('<span id=\"texto-review-')\n",
        "  parsed_html = html[review_span_idx:]\n",
        "  review_start = parsed_html.index(\">\")\n",
        "  review_end = parsed_html.index(\"</span>\")\n",
        "  review = parsed_html[review_start + 1: review_end]\n",
        "\n",
        "  return stars, review"
      ],
      "metadata": {
        "id": "DqXVuP2d1bDy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def save_list_to_json(obj, path):\n",
        "  # Convert list to JSON string\n",
        "  json_data = json.dumps(obj, indent=4)\n",
        "\n",
        "  # Save JSON to a file\n",
        "  with open(path, \"w\") as json_file:\n",
        "      json_file.write(json_data)\n",
        "\n",
        "def load_list_from_json(path):\n",
        "  with open(path, \"r\") as json_file:\n",
        "      return json.load(json_file)"
      ],
      "metadata": {
        "id": "QAxec3qmzabw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_links_local_path = \"/content/book_links.json\"\n",
        "labeled_data_local_path = \"/content/labeled_data.json\""
      ],
      "metadata": {
        "id": "pluOEz2L5x3m"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_from_file = True\n",
        "\n",
        "if load_from_file:\n",
        "  labeled_data =load_list_from_json(labeled_data_local_path)\n",
        "else:\n",
        "  labeled_data = []\n",
        "\n",
        "book_links = set(load_list_from_json(book_links_local_path))"
      ],
      "metadata": {
        "id": "3r665sIB0_LD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "total_book_links = len(book_links)\n",
        "batch_size = 50\n",
        "num_batches = math.ceil(total_book_links / batch_size)"
      ],
      "metadata": {
        "id": "gf4XCtoqzKdD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7V1c4VAp37M",
        "outputId": "72e09380-f798-4120-dfc7-67c0366dc250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "25\n",
            "50\n"
          ]
        }
      ],
      "source": [
        "labeled_data = []\n",
        "\n",
        "for j in range(0, num_batches):\n",
        "    start = j * batch_size\n",
        "\n",
        "    for i, book_link in enumerate(sorted(list(book_links))[start:start + batch_size]):\n",
        "        if (i % 25 == 0): print(start + i)\n",
        "\n",
        "        try:\n",
        "            response = requests.get(book_link, headers=headers, timeout=5)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Grab title (assuming it's still inside the static HTML)\n",
        "            title_elem = soup.find(class_=\"tituloProducto\")\n",
        "            if not title_elem:\n",
        "                print(\"No title found\")\n",
        "                continue\n",
        "            title = title_elem.get_text(strip=True)\n",
        "\n",
        "            # Reviews (limited to reviews already on the page)\n",
        "            review_substring = \"review-n-\"\n",
        "            review_elements = soup.find_all(class_=lambda x: x and review_substring in x)\n",
        "\n",
        "            for review_element in review_elements:\n",
        "                review_html = review_element.decode_contents()\n",
        "                stars, review = get_review_data_from_html(review_html)\n",
        "                labeled_data.append(\n",
        "                    {\n",
        "                        \"rating\": stars,\n",
        "                        \"review\": review,\n",
        "                        \"title\": title,\n",
        "                    }\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {book_link}: {e}\")\n",
        "            continue\n",
        "    save_list_to_json(labeled_data, labeled_data_local_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_list_to_json(labeled_data, labeled_data_local_path)"
      ],
      "metadata": {
        "id": "sL2-fIi07Ugj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len (labeled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jjXai4HJq3T",
        "outputId": "a2477b7f-3697-49bb-e934-f77b9c041522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12140"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saw79CjQkhnJ",
        "outputId": "171b81af-c447-4d5e-b30d-22edfaa9cc89"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We good for the ROBOTS.TXT-- https://www.buscalibre.cl/robots.txt"
      ],
      "metadata": {
        "id": "CwGqfqmhydWd"
      }
    }
  ]
}