{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k_FImvijKxa"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from urllib.robotparser import RobotFileParser\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
        "    'Connection': 'keep-alive'\n",
        "}"
      ],
      "metadata": {
        "id": "Mo6B5B1GACT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_not_allowed(rp, url):\n",
        "    # return not rp.can_fetch('*', url)\n",
        "    return False"
      ],
      "metadata": {
        "id": "jMKNpo1wr6r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from requests.exceptions import RequestException"
      ],
      "metadata": {
        "id": "eMwEslqsvyEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_url(base_url, rp, url, depth):\n",
        "    with lock:\n",
        "        if url in visited_urls or depth > max_depth or is_not_allowed(rp, url):\n",
        "            return []\n",
        "\n",
        "        visited_urls.add(url)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1', class_='jeg_post_title')\n",
        "        if not title:\n",
        "            pass\n",
        "        else: # only record text if recieved title\n",
        "            title_text = title.get_text(strip=True)\n",
        "            new_texts = []\n",
        "            paragraphs = soup.find_all('p')\n",
        "            for p in paragraphs:\n",
        "                text = p.get_text(strip=True)\n",
        "                if len(text) > 150:\n",
        "                    new_texts.append(text)\n",
        "\n",
        "            with lock:\n",
        "                all_text[url] = {\n",
        "                    \"title\": title_text,\n",
        "                    \"text\": new_texts\n",
        "                }\n",
        "\n",
        "        new_links = []\n",
        "        links = soup.find_all('a')\n",
        "        for link in links:\n",
        "            href = link.get('href')\n",
        "            if href and (\n",
        "                  href.startswith(\"/\") or href.startswith(url)\n",
        "                  or (not href.startswith(\"https://\") and not href.startswith(\"javascript\"))\n",
        "            ):\n",
        "                next_url = urljoin(url, href)\n",
        "                with lock:\n",
        "                    if next_url not in visited_urls:\n",
        "                        new_links.append((next_url, depth + 1))\n",
        "\n",
        "        return new_links\n",
        "\n",
        "    except RequestException as e:\n",
        "        if e.response and e.response.status_code == 403:\n",
        "            print(f\"403 Forbidden on {url}\")\n",
        "        return []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unhandled error on {url}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "Y6HnAMP34bIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_depth = 10\n",
        "max_count = int(1e5)\n",
        "max_workers = 16"
      ],
      "metadata": {
        "id": "35Ra1CPpsFZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "speed experiments for crawling 10k links\n",
        "* max_workers = 8: 327.23 seconds\n",
        "* max_workers = 16: 234.79 seconds"
      ],
      "metadata": {
        "id": "T-qJrk-xCVcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_local_path = \"/content/fotech_output.json\""
      ],
      "metadata": {
        "id": "RanxVCxX8uNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists(text_local_path):\n",
        "    with open(text_local_path, 'r') as f:\n",
        "        output_dict = json.load(f)\n",
        "    print(\"Loaded existing text_output.json\")\n",
        "else:\n",
        "  output_dict = {}"
      ],
      "metadata": {
        "id": "uACban1m_B5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503c8601-2f73-43d3-bc7a-2943fa0a36ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing text_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.fotech.cl/\""
      ],
      "metadata": {
        "id": "zHELR-vYtqA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import json\n",
        "import time\n",
        "lock = threading.Lock()"
      ],
      "metadata": {
        "id": "ETJuTGgZ4Q0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visited_urls = set()\n",
        "queue = [(base_url, 0)]\n",
        "\n",
        "\n",
        "robots_url = urljoin(base_url, '/robots.txt')\n",
        "rp = RobotFileParser()\n",
        "rp.set_url(robots_url)\n",
        "rp.read()\n",
        "\n",
        "queue = deque(queue)\n",
        "count = 0\n",
        "MAX_CONSECUTIVE_TIMEOUTS = 10\n",
        "consecutive_timeouts = 0\n",
        "\n",
        "all_text = {}\n",
        "executor = ThreadPoolExecutor(max_workers=max_workers)\n",
        "futures = set()\n",
        "try:\n",
        "    while queue and count < max_count:\n",
        "        while queue and len(futures) < max_workers:\n",
        "            url, depth = queue.popleft()\n",
        "            future = executor.submit(process_url, base_url, rp, url, depth)\n",
        "            futures.add(future)\n",
        "            count += 1\n",
        "            if count % 1000 == 0:\n",
        "                print(\"log: \", count, len(queue))\n",
        "\n",
        "        done, futures = wait(futures, timeout=10, return_when='FIRST_COMPLETED')\n",
        "        if not done:\n",
        "            consecutive_timeouts += 1\n",
        "            print(f\"Timeout #{consecutive_timeouts}: no futures completed this cycle.\")\n",
        "            if consecutive_timeouts >= MAX_CONSECUTIVE_TIMEOUTS:\n",
        "                print(\"Too many consecutive timeouts! Exiting crawl early.\")\n",
        "                break\n",
        "        else:\n",
        "            consecutive_timeouts = 0\n",
        "\n",
        "        for future in done:\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                queue.extend(result)\n",
        "\n",
        "finally:\n",
        "    # Cancel any unfinished futures before shutdown\n",
        "    for f in futures:\n",
        "        if not f.done():\n",
        "            f.cancel()\n",
        "    executor.shutdown(wait=False)  # Safe to call here\n",
        "\n",
        "with open(text_local_path, 'w') as f:\n",
        "    json.dump(all_text, f)\n",
        "print(len(all_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiF6sXAS4s9d",
        "outputId": "d2afc729-c9ef-43f8-8a17-5d57b0116b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log:  1000 32\n",
            "log:  2000 1285\n",
            "log:  3000 2151\n",
            "log:  4000 2351\n",
            "log:  5000 2822\n",
            "log:  6000 2990\n",
            "log:  7000 3311\n",
            "log:  8000 3231\n",
            "log:  9000 3139\n",
            "log:  10000 3064\n",
            "log:  11000 2668\n",
            "log:  12000 1978\n",
            "log:  13000 979\n",
            "117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(all_text)"
      ],
      "metadata": {
        "id": "nyj7aRp49c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf982676-38fc-4a5b-eecd-2ac23ccc2028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}