{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "5k_FImvijKxa"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from urllib.robotparser import RobotFileParser\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_not_allowed(rp, url):\n",
        "    return not rp.can_fetch('*', url)"
      ],
      "metadata": {
        "id": "jMKNpo1wr6r9"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_url(base_url, rp, url, depth):\n",
        "    with lock:\n",
        "        if url in visited_urls or depth > max_depth or is_not_allowed(rp, url):\n",
        "            return []\n",
        "\n",
        "        visited_urls.add(url)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        new_texts = []\n",
        "        paragraphs = soup.find_all('p')\n",
        "        for p in paragraphs:\n",
        "            text = p.get_text(strip=True)\n",
        "            if len(text) > 150:\n",
        "                new_texts.append(text)\n",
        "\n",
        "        with lock:\n",
        "            for text in new_texts:\n",
        "                all_text.add(text)\n",
        "\n",
        "        new_links = []\n",
        "        links = soup.find_all('a')\n",
        "        for link in links:\n",
        "            href = link.get('href')\n",
        "            if href and (\n",
        "                  href.startswith(\"/\") or href.startswith(url)\n",
        "            ):\n",
        "                next_url = urljoin(url, href)\n",
        "                with lock:\n",
        "                    if next_url not in visited_urls:\n",
        "                        new_links.append((next_url, depth + 1))\n",
        "\n",
        "        return new_links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on {url}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "Y6HnAMP34bIp"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_depth = 5\n",
        "max_count = int(1e5)\n",
        "max_workers = 16"
      ],
      "metadata": {
        "id": "35Ra1CPpsFZ3"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "speed experiments for crawling 10k links\n",
        "* max_workers = 8: 327.23 seconds\n",
        "* max_workers = 16: 234.79 seconds"
      ],
      "metadata": {
        "id": "T-qJrk-xCVcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_local_path = \"/content/text_output.json\""
      ],
      "metadata": {
        "id": "RanxVCxX8uNT"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists(text_local_path):\n",
        "    with open(text_local_path, 'r') as f:\n",
        "        output_dict = json.load(f)\n",
        "    print(\"Loaded existing text_output.json\")\n",
        "else:\n",
        "  output_dict = {}"
      ],
      "metadata": {
        "id": "uACban1m_B5t"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://www.fotech.cl/\",\n",
        "    \"https://www.portalnet.cl/\",\n",
        "    \"https://www.gamba.cl/\",\n",
        "    \"https://www.soychile.cl/\",\n",
        "    \"https://www.df.cl/\",\n",
        "    \"https://www.ed.cl/\",\n",
        "    \"https://www.wwf.cl/\",\n",
        "    \"https://www.elmostrador.cl/\",\n",
        "    \"https://www.biobiochile.cl/\",\n",
        "    \"https://fundacionsol.cl/\",\n",
        "    \"https://observatoriodesigualdades.udp.cl/\",\n",
        "]"
      ],
      "metadata": {
        "id": "zHELR-vYtqA7"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import json\n",
        "import time\n",
        "lock = threading.Lock()"
      ],
      "metadata": {
        "id": "ETJuTGgZ4Q0w"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visited_urls = set()\n",
        "multi_level_queue = {}\n",
        "for url in urls:\n",
        "  multi_level_queue[url] = [(url, 0)]\n",
        "\n",
        "start = time.time()\n",
        "for base_url, queue in multi_level_queue.items():\n",
        "    print(\"Crawling:\", base_url)\n",
        "\n",
        "    robots_url = urljoin(base_url, '/robots.txt')\n",
        "    rp = RobotFileParser()\n",
        "    rp.set_url(robots_url)\n",
        "    rp.read()\n",
        "\n",
        "    queue = deque(queue)\n",
        "    count = 0\n",
        "\n",
        "    all_text = set()\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = set()\n",
        "\n",
        "        while queue and count < max_count:\n",
        "            while queue and len(futures) < max_workers:\n",
        "                url, depth = queue.popleft()\n",
        "                future = executor.submit(process_url, base_url, rp, url, depth)\n",
        "                futures.add(future)\n",
        "                count += 1\n",
        "                if count % 1000 == 0:\n",
        "                    print(\"log: \", count, len(queue))\n",
        "\n",
        "            done, futures = wait(futures, return_when='FIRST_COMPLETED')\n",
        "            for future in done:\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    queue.extend(result)\n",
        "    output_dict[base_url] = list(all_text)\n",
        "\n",
        "    with open(text_local_path, 'w') as f:\n",
        "      json.dump(output_dict, f)\n",
        "\n",
        "print(f\"Time taken: {time.time() - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiF6sXAS4s9d",
        "outputId": "088d41e8-a7ec-4291-fe72-e028221a49bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://www.fotech.cl/\n",
            "Crawling: https://www.portalnet.cl/\n",
            "Crawling: https://www.gamba.cl/\n",
            "Error on https://www.gamba.cl/: 403 Client Error: Forbidden for url: https://www.gamba.cl/\n",
            "Crawling: https://www.soychile.cl/\n",
            "Error on https://www.soychile.cl/: 403 Client Error: Forbidden for url: https://www.soychile.cl/\n",
            "Crawling: https://www.df.cl/\n",
            "log:  1000 28261\n",
            "log:  2000 28199\n",
            "log:  3000 27870\n",
            "log:  4000 28952\n",
            "Error on https://www.df.cl/legales/site/tax/port/all/taxport_24___1.html: 404 Client Error: Not Found for url: https://www.df.cl/legales/site/tax/port/all/taxport_24___1.html\n",
            "log:  5000 28630\n",
            "log:  6000 28839\n",
            "log:  7000 28322\n",
            "log:  8000 27684\n",
            "log:  9000 27459\n",
            "log:  10000 27160\n",
            "log:  11000 26842\n",
            "log:  12000 26516\n",
            "log:  13000 26163\n",
            "log:  14000 25510\n",
            "log:  15000 25116\n",
            "log:  16000 24311\n",
            "log:  17000 23574\n",
            "log:  18000 22574\n",
            "log:  19000 21599\n",
            "log:  20000 20770\n",
            "log:  21000 20001\n",
            "log:  22000 19001\n",
            "log:  23000 18178\n",
            "log:  24000 17182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyj7aRp49c80"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}